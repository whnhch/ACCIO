{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eq1SpIqM_Vwt",
    "outputId": "d1097bef-33c3-4817-90ba-22f3aa9f7352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'cta' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/whnhch/cta.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu1472329\u001b[0m (\u001b[33mwhanhee\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages/numpy-1.22.3-py3.10-linux-x86_64.egg (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /uufs/chpc.utah.edu/common/home/u1472329/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /uufs/chpc.utah.edu/sys/installdir/r8/python/3.10.3/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LEAyqnbg_VFt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from cta.utils.data import MyDataset\n",
    "import sys\n",
    "import random\n",
    "from cta.models import TabCSE\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "\n",
    "lr=1e-5\n",
    "epochs=10\n",
    "bs=64\n",
    "model_name='bert-base-uncased'\n",
    "max_seq=256\n",
    "temp=0.05\n",
    "data_path='dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: /sys/module/amdgpu/initstate: No such file or directory\n",
      "ERROR:root:Driver not initialized (amdgpu not found in modules)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/uufs/chpc.utah.edu/common/home/u1472329/nlp_2024/wandb/run-20240420_195535-a9ehso6p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/whanhee/nlp-2024-tabcse/runs/a9ehso6p' target=\"_blank\">happy-bird-39</a></strong> to <a href='https://wandb.ai/whanhee/nlp-2024-tabcse' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/whanhee/nlp-2024-tabcse' target=\"_blank\">https://wandb.ai/whanhee/nlp-2024-tabcse</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/whanhee/nlp-2024-tabcse/runs/a9ehso6p' target=\"_blank\">https://wandb.ai/whanhee/nlp-2024-tabcse/runs/a9ehso6p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/whanhee/nlp-2024-tabcse/runs/a9ehso6p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f493eaefbb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"nlp-2024-tabcse\",\n",
    "    config={\n",
    "        \"batch_size\": bs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"dataset\": \"table-pivot\",\n",
    "        \"temp\": temp,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zaq6JiKm-iV2"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TabCSE(model_name, temp)\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "mydataset = MyDataset(data['data'],data['pivot'], max_seq, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_4KtmV9WhejM"
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch Dataset\n",
    "input_ids = torch.tensor(mydataset.x['input_ids'])\n",
    "attention_masks = torch.tensor(mydataset.x['attention_mask'])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids,attention_masks)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O5q-aDet-5C4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGxb4n7JBR4b",
    "outputId": "cd08bd79-ba61-4e4a-849e-faa422178b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iteration [30/103], Average Loss: 2.5927\n",
      "Epoch [1/10], Iteration [60/103], Average Loss: 1.3456\n",
      "Epoch [1/10], Iteration [90/103], Average Loss: 1.1244\n",
      "Epoch [2/10], Iteration [30/103], Average Loss: 0.9146\n",
      "Epoch [2/10], Iteration [60/103], Average Loss: 0.8713\n",
      "Epoch [2/10], Iteration [90/103], Average Loss: 0.8504\n",
      "Epoch [3/10], Iteration [30/103], Average Loss: 0.7653\n",
      "Epoch [3/10], Iteration [60/103], Average Loss: 0.7500\n",
      "Epoch [3/10], Iteration [90/103], Average Loss: 0.7108\n",
      "Epoch [4/10], Iteration [30/103], Average Loss: 0.6551\n",
      "Epoch [4/10], Iteration [60/103], Average Loss: 0.7067\n",
      "Epoch [4/10], Iteration [90/103], Average Loss: 0.6618\n",
      "Epoch [5/10], Iteration [30/103], Average Loss: 0.6345\n",
      "Epoch [5/10], Iteration [60/103], Average Loss: 0.6235\n",
      "Epoch [5/10], Iteration [90/103], Average Loss: 0.6100\n",
      "Epoch [6/10], Iteration [30/103], Average Loss: 0.5985\n",
      "Epoch [6/10], Iteration [60/103], Average Loss: 0.5716\n",
      "Epoch [6/10], Iteration [90/103], Average Loss: 0.6182\n",
      "Epoch [7/10], Iteration [30/103], Average Loss: 0.5339\n",
      "Epoch [7/10], Iteration [60/103], Average Loss: 0.5893\n",
      "Epoch [7/10], Iteration [90/103], Average Loss: 0.5855\n",
      "Epoch [8/10], Iteration [30/103], Average Loss: 0.5275\n",
      "Epoch [8/10], Iteration [60/103], Average Loss: 0.5528\n",
      "Epoch [8/10], Iteration [90/103], Average Loss: 0.5848\n",
      "Epoch [9/10], Iteration [30/103], Average Loss: 0.5799\n",
      "Epoch [9/10], Iteration [60/103], Average Loss: 0.5387\n",
      "Epoch [9/10], Iteration [90/103], Average Loss: 0.5335\n",
      "Epoch [10/10], Iteration [30/103], Average Loss: 0.5456\n",
      "Epoch [10/10], Iteration [60/103], Average Loss: 0.5467\n",
      "Epoch [10/10], Iteration [90/103], Average Loss: 0.5357\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        x_input_ids, x_attention_mask = batch\n",
    "        x_input_ids, x_attention_mask = x_input_ids.to(device), x_attention_mask.to(device)\n",
    "\n",
    "        z1, z2 = model(x_input_ids, x_attention_mask)\n",
    "\n",
    "        cos_sim = model.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(device)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(cos_sim, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 30 == 0:\n",
    "            average_loss = epoch_loss / 30\n",
    "            print(\"Epoch [{}/{}], Iteration [{}/{}], Average Loss: {:.4f}\".format(epoch + 1, epochs, idx+ 1, len(dataloader), average_loss))\n",
    "            wandb.log({\"tabcse_loss\": average_loss})\n",
    "            epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ysn45IXA92Iy"
   },
   "outputs": [],
   "source": [
    "# Define a file path for saving the checkpoint\n",
    "checkpoint_path = 'tabcse_checkpoint.pth'\n",
    "\n",
    "# Create a dictionary to save the checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}\n",
    "\n",
    "# Save the checkpoint to the file\n",
    "torch.save(checkpoint, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
